---
title: Terminal ASCII Image Generator
date: 'April 19th, 2025'
description: Detailing my steps through writing a program that converts images to ASCII art and displaying it in the terminal.
---

# Introduction
If you've spent any time browsing the internet back in the early 2000's, you've probably come across a number of ASCII images. While some images are simple and can probably be reconstructed by hand, others are incredibly complex; I mean, there's no way somebody sat there and manually typed out something like the Mona Lisa, right?

![Mona Lisa ASCII Art](/blog/ascii/mona_lisa.jpg "Mona Lisa ASCII Art")

Obviously not -- clearly, there was some programming to convert these images into ASCII. Thus, curious about this process, I decided to tinker around and build my own ASCII converter.

For the rest of this post, I'll be detailing how I went about creating an image-to-ASCII converter, along with implementing refinements to the output.

For the purposes of this post, we will be using a picture of beloved Pok&#233;mon mascot, Pikachu:

![Pikachu](/blog/ascii/pikachu.png "Pikachu")

# Na&iumlve Approach
The core idea is that we want to replace each pixel in our image with an ASCII character. 

The question then is what feature of the pixels do we want each character to convey. One natural choice is to try conveying the ``intensity" of a pixel -- that is, how bright a pixel is. We can extract this intensity $Y$ by converting our image to a grayscale version of itself using the following formula:

$$
Y = 0.2126R + 0.7152G + 0.0722B
$$

For my na&iumlve implementation, I'll be using the following ten characters:
```python
ASCII_LIST = [" ", ".", ":", "c", "o", "P", "O", "?", "@", "■"]
```

The characters here are listed in ascending order of how intense they are. This was done by just eyeballing how much blank space there is for the characters -- the less, the brighter.

However, we now have an issue: there are way more intensity values than there are ASCII characters! The solution then is that we can construct bins to place our intensity values into. In our case, since we have ten ASCII characters, we can create ten equally-sized bins and map brightness values that falls within each bin to their corresponding ASCII character.

## Implementation
With all of that in mind, we can begin coding this up in Python. First, we will define a `grayscale()` function that takes in an iamge and grayscale it. This can be done simply using the `PIL` library:

```python
def grayscale(image: PIL.Image.Image) -> PIL.Image.Image:
    return image.convert("L")
```

Now, we want to actually extract our image given an image path, and then use our `grayscale()` function that we wrote to extract the intensity of each pixel:
```python
path = input("Enter a valid path to an image:\n")
try:
    image = PIL.Image.open(path)
except:
    print("Error opening image.")
    return

grayscaled_image = grayscale(image)
```

With our grayscaled image, we are now ready to convert it into an ASCII picture. The steps are as follows:
- First, we will get the pixel data for our grayscaled image using `image.getdata()`.
- Now, we iterate through each pixel, looking at their intensity.
    - Since $Y \in [0, 255]$, we will first scale it down so that we have $Y \in [0, 1]$. This can be done by dividing a pixel's brightness value by $255$.
- Next, we assign the pixel to their corresponding bucket using the following equation:
$$
\max\left\{0, \lfloor Y \times \texttt{len(ASCII\_LIST)}\rfloor - 1\right\}
$$
- Then, we append each of these characters to a list `asciified` representing our ASCII image.
- Finally, we iterate through this list, adding in line breaks as necessary.

Writing it out in code, we would get something along these lines:
```python
pixels = image.getdata()
asciified = []

for pixel in pixels:
    luminance = pixel / 255
    bucket = max(math.floor(luminance * len(ASCII_LIST)) - 1, 0)
    asciified.append(ASCII_LIST[bucket])

characters = "".join(asciified)
ascii = "\n".join(characters[i:(i + image.width)] for i in range(0, len(characters), image.width))
```

Finally, as a bit of an extra step, we can save our result into a `.txt` file using the `os` library:
```python
with open("ascii_img.txt", "w", encoding="utf-8") as f:
    f.write(ascii)
os.startfile("ascii_img.txt")
```

Putting this all together, we get:
```python
import math
import PIL.Image
import os

ASCII_LIST = [" ", ".", ":", "c", "o", "P", "O", "?", "@", "■"]

def grayscale(image: PIL.Image.Image) -> PIL.Image.Image:
    return image.convert("L")

def asciify(image: PIL.Image.Image) -> str:
    pixels = image.getdata()
    asciified = []

    for pixel in pixels:
        luminance = pixel / 255
        bucket = max(math.floor(luminance * len(ASCII_LIST)) - 1, 0)
        asciified.append(ASCII_LIST[bucket])

    characters = "".join(asciified)
    ascii = "\n".join(characters[i:(i + image.width)] for i in range(0, len(characters), image.width))

    return ascii

def main():
    path = input("Enter a valid path to an image:\n")
    try:
        image = PIL.Image.open(path)
    except:
        print("Error opening image.")

    grayscaled_image = grayscale(image)

    ascii = asciify(grayscaled_image)

    with open("ascii_img.txt", "w", encoding="utf-8") as f:
        f.write(ascii)
    os.startfile("ascii_img.txt")

main()
```

Running this program on my profile picture, we get the following result in our `.txt` file:

![Profile Picture ASCII (Na&iumlve)](/blog/ascii/pikachu_ascii_naive.png "Profile Picture ASCII (Na&iumlve)")

Note that the image was so large that it wouldn't even fit into our terminal.

## Issues
There are a few issues with the current implementation:
- **Deformations**: One of the most obvious issues is how the image looks like it has been stretched vertically. This is a result of there being spaces between each of our lines and characters; we will have to account for this when converting our image to ASCII.
- **Loss of Detail**: For more complex pictures, we simply lose out on too much detail and it becomes hard to determine what's going on in the image. Some possible fixes are as follows:
    - *Colors*: Adding in color may better help us differentiate between different parts of the image.
    - *Edge Detection*: We will discuss more on this later.

For the remainder of this blog, we will be tackling ways to handle these issues.

# Fixes: Deformations
The first thing we will fix is the deformations. The main issue here is that because we aren't accounting for the gaps caused by the vertical space between each character, the image becomes a lot stretched out.

A simple way of fixing this is to simply eyeball it, changing the aspect ratio of our ASCII art to accomodate these gaps. While doing this, we can also implement a resizing function so we don't have a massive image that doesn't even fit into our terminal:
```python
def resize_image(image: PIL.Image.Image, new_width:int=-1) -> PIL.Image.Image:
    width, height = image.size

    if new_width == -1:
        resized = image.resize((width, height // 2))
        return resized
    
    aspect_ratio = height / width
    new_height = int(aspect_ratio * new_width) // 2
    resized = image.resize((new_width, new_height))
    
    return resized
```

With this resizing, we now get a much better result (in our terminal, too!):

![Vertical Deformation Fix](/blog/ascii/pikachu_deformation_fix.png "Vertical Deformation Fix")

# Fixes: Color
One of the next improvements we can implement is adding color to our image. This would help us differentiate between certain parts of the image easier, not just relying on the brightness value of a pixel. However, we run into an issue very quickly -- many older terminals only allow eight colors.

Below, we see a list of colors that most terminals offer; note that older ones can only support the first eight:

![ANSI Terminal Colors](/blog/ascii/ansi_colors.png "Available Terminal Colors")

On the other hand, since images use $24$-bit colors, this means we have $2^{24}$ possible colors... that's *a lot* more than sixteen.

The question then is how we can exactly represent all of these colors using a set of $8$ colors only. The solution is relatively simple: we note that since each pixel is represented as a tuple of three elements $(R, G, B)$, we can treat this as a vector. Then, for each color in our image, we find the vector closest to it from our available eight colors.

A natural choice of metric for determining closeness would be the Euclidean distance between two colors. Let $c$ be the color of the pixel we're looking at, and $c'$ to in the set of our available colors. We want to find the closest color $c^*$ as such:
$$
\begin{equation*}
    c^* = \mathrm{argmin}_{c'} \sqrt{(c - c')^2}
\end{equation*}
$$

## Implementation
To begin with, we will be using the `colorama` library in order to have colored output in our terminal. We will also construct a list of our available colors' RGB values, their `colorama` name, and a mapping between RGB value and name:
```python
from colorama import Fore, just_fix_windows_console

# ASCII Character Set to use
ASCII_LIST = [" ", ".", ":", "c", "o", "P", "O", "?", "@", "■"]
# RGB value of our ANSI colors
COLORS = [(i, j, k, 255) for i in range(0, 256, 255) for j in range (0, 256, 255) for k in range(0, 256, 255)]
# ANSI code
ANSI = [Fore.BLACK, Fore.BLACK, Fore.GREEN, Fore.CYAN, Fore.RED, Fore.MAGENTA, Fore.YELLOW, Fore.WHITE]
# Mapping between RGB to ANSI
MAP = dict(zip(COLORS, ANSI))
```

Next, we create helper functions to calculate the closest color we can use to represent any arbitrary color:
```python
# Calculates the Euclidean Distance between two RGB tuples
def euclidean_distance(color1: tuple, color2: tuple) -> float:
    return math.sqrt(sum((c1 - c2)**2 for c1, c2 in zip(color1, color2)))

# Returns closest ANSI color to a pixel
def closest_color(color):
    min_dist = float("inf")
    min_color = None
    for c in COLORS:
        dist = euclidean_distance(c, color)
        if dist < min_dist:
            min_dist = dist
            min_color = c
    
    return MAP[min_color]
```

Finally, we update our `asciify()` function so that it will also add in color information. Because we are now using color as well, we will be iterating over the colored image's pixel data rather than a grayscaled version; with that in mind, we instead calculate the intensity value directly using a new helper function `get_intensity()`:
```python
def get_intensity(pixel: tuple):
    return (0.299 * pixel[0]) + (0.587 * pixel[1]) + (0.114 * pixel[2])

# Converts a grayscaled PIL.Image.Image instance into ASCII
def asciify(image: PIL.Image.Image, width: int=-1) -> str:
    image = resize_image(image, width)
    pixels = image.getdata()
    ascii = []
    i = 0

    for pixel in pixels:
        # First, we calculate the luminance by transforming the pixel value so that it's between 0 and 1. Since each pixel value ranges from 0 to 255, this can be done by dividing it by 255.
        luminance = get_intensity(pixel) / 255
        # Next, we place the luminance into the corresponding ASCII "bucket". We have to -1 from this though so it fits into our buckets.
        bucket = max(math.floor(luminance * len(ASCII_LIST)) - 1, 0)
        # Now, we find the closest color to our pixel.
        ansi = closest_color(pixel)
        
        if i % width == 0:
            ch = f"{ansi}{ASCII_LIST[bucket]}\n"
        else:
            ch = f"{ansi}{ASCII_LIST[bucket]}"
        i += 1
        ascii.append(ch)

    ascii.append(Fore.RESET)
    res = "".join(ascii)
    return res
```

Note that we have also added a new variable to keep track of when we need a linebreak or not; this is to help reduce the number of times we need to call the `.join()` method for some minor speed-up.

With this improvement, we can now run it on our profile picture and see that we get the following image:

![Pikachu Colored](/blog/ascii/pikachu_colored_naive.png "Pikachu Colored")